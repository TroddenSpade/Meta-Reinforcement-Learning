{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tEkxDmPoMysA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common\n",
        "\n",
        "!apt-get install -y patchelf\n",
        "!pip install free-mujoco-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 919
        },
        "id": "wm1DtAFVgZkh",
        "outputId": "d0b81090-3eb5-4e74-8430-a8c92fa9a892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libglew-dev is already the newest version (2.0.0-5).\n",
            "libgl1-mesa-dev is already the newest version (20.0.8-0ubuntu1~18.04.1).\n",
            "libgl1-mesa-glx is already the newest version (20.0.8-0ubuntu1~18.04.1).\n",
            "libosmesa6-dev is already the newest version (20.0.8-0ubuntu1~18.04.1).\n",
            "software-properties-common is already the newest version (0.96.24.32.18).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "patchelf is already the newest version (0.9-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting free-mujoco-py\n",
            "  Downloading free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners==0.15\n",
            "  Downloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from free-mujoco-py) (1.15.1)\n",
            "Requirement already satisfied: Cython<0.30.0,>=0.29.24 in /usr/local/lib/python3.8/dist-packages (from free-mujoco-py) (0.29.32)\n",
            "Collecting glfw<2.0.0,>=1.4.0\n",
            "  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 KB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.8/dist-packages (from free-mujoco-py) (1.21.6)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from free-mujoco-py) (2.9.0)\n",
            "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.8/dist-packages (from fasteners==0.15->free-mujoco-py) (1.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from fasteners==0.15->free-mujoco-py) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.21)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (7.1.2)\n",
            "Installing collected packages: glfw, fasteners, free-mujoco-py\n",
            "  Attempting uninstall: glfw\n",
            "    Found existing installation: glfw 2.5.5\n",
            "    Uninstalling glfw-2.5.5:\n",
            "      Successfully uninstalled glfw-2.5.5\n",
            "  Attempting uninstall: fasteners\n",
            "    Found existing installation: fasteners 0.18\n",
            "    Uninstalling fasteners-0.18:\n",
            "      Successfully uninstalled fasteners-0.18\n",
            "Successfully installed fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "fasteners"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW5mWnSu4AKa"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 &> /dev/null\n",
        "!pip install gym==0.21.0 &> /dev/null\n",
        "!pip install cherry-rl &> /dev/null\n",
        "!pip install learn2learn &> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Envs"
      ],
      "metadata": {
        "id": "tEkxDmPoMysA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2D Navigation"
      ],
      "metadata": {
        "id": "twkngiIoeqdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "\n",
        "\n",
        "class Navigation2DEnv(gym.Env):\n",
        "    \"\"\"2D navigation problems, as described in [1]. The code is adapted from \n",
        "    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/maml_examples/point_env_randgoal.py\n",
        "\n",
        "    At each time step, the 2D agent takes an action (its velocity, clipped in\n",
        "    [-0.1, 0.1]), and receives a penalty equal to its L2 distance to the goal \n",
        "    position (ie. the reward is `-distance`). The 2D navigation tasks are \n",
        "    generated by sampling goal positions from the uniform distribution \n",
        "    on [-0.5, 0.5]^2.\n",
        "\n",
        "    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, \"Model-Agnostic \n",
        "        Meta-Learning for Fast Adaptation of Deep Networks\", 2017 \n",
        "        (https://arxiv.org/abs/1703.03400)\n",
        "    \"\"\"\n",
        "    def __init__(self, task={}, low=-0.5, high=0.5):\n",
        "        super(Navigation2DEnv, self).__init__()\n",
        "        self.low = low\n",
        "        self.high = high\n",
        "\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
        "            shape=(2,), dtype=np.float32)\n",
        "        self.action_space = spaces.Box(low=-0.1, high=0.1,\n",
        "            shape=(2,), dtype=np.float32)\n",
        "\n",
        "        self._task = task\n",
        "        self._goal = task.get('goal', np.zeros(2, dtype=np.float32))\n",
        "        self._state = np.zeros(2, dtype=np.float32)\n",
        "        self.seed()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def sample_tasks(self, num_tasks):\n",
        "        goals = self.np_random.uniform(self.low, self.high, size=(num_tasks, 2))\n",
        "        tasks = [{'goal': goal} for goal in goals]\n",
        "        return tasks\n",
        "\n",
        "    def reset_task(self, task):\n",
        "        self._task = task\n",
        "        self._goal = task['goal']\n",
        "\n",
        "    def reset(self, env=True):\n",
        "        self._state = np.zeros(2, dtype=np.float32)\n",
        "        return self._state\n",
        "\n",
        "    def step(self, action):\n",
        "        action = np.clip(action, -0.1, 0.1)\n",
        "        assert self.action_space.contains(action)\n",
        "        self._state = self._state + action\n",
        "\n",
        "        x = self._state[0] - self._goal[0]\n",
        "        y = self._state[1] - self._goal[1]\n",
        "        reward = -np.sqrt(x ** 2 + y ** 2)\n",
        "        done = ((np.abs(x) < 0.01) and (np.abs(y) < 0.01))\n",
        "\n",
        "        return self._state, reward, done, {'task': self._task}"
      ],
      "metadata": {
        "id": "N2VRWpZ84L3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = Navigation2DEnv()"
      ],
      "metadata": {
        "id": "OeDtxren5D6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvvrIXHC5JQ-",
        "outputId": "4c4e7abd-489c-4810-fab6-94ea766c2d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeA2A_Ol5NQY",
        "outputId": "299c8179-4f64-4e01-dc8a-4c6b292db677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(-0.1, 0.1, (2,), float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space.sample()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgA5Olrr6AyV",
        "outputId": "b9d20d7f-9166-487e-9bfa-7d72cd7f2127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.05877567, -0.03047173], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.step(env.action_space.sample())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDQQZ_Uw5j8Y",
        "outputId": "81678a5f-b536-42e1-a9d7-df9c021b8b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.04863435, 0.04859379], dtype=float32),\n",
              " -0.06875068547855158,\n",
              " False,\n",
              " {'task': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cheetah"
      ],
      "metadata": {
        "id": "5-C9MOdNeuCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import learn2learn as l2l\n",
        "\n",
        "env = gym.make('HalfCheetahForwardBackward-v1')"
      ],
      "metadata": {
        "id": "H7D9V6fTev-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXVlUhE-gKzB",
        "outputId": "543186a5-bb97-426a-fffc-59048deef232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.00214652,  0.01273377,  0.01084169, -0.09718947,  0.09503878,\n",
              "       -0.04999675,  0.06876124, -0.08774628, -0.11113398,  0.11305978,\n",
              "        0.02529547,  0.05023619,  0.06900839,  0.22366214,  0.14268471,\n",
              "       -0.06499392,  0.03702499, -0.08593929,  0.        ,  0.69785345],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Networks"
      ],
      "metadata": {
        "id": "fHKWBF3RcsT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import Normal, Categorical\n",
        "\n",
        "EPSILON = 1e-6\n",
        "\n",
        "\n",
        "def linear_init(module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        nn.init.xavier_uniform_(module.weight)\n",
        "        module.bias.data.zero_()\n",
        "    return module"
      ],
      "metadata": {
        "id": "0QkxcGZFcxVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiagNormalPolicy(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hiddens=None, activation='relu', device='cpu'):\n",
        "        super(DiagNormalPolicy, self).__init__()\n",
        "        self.device = device\n",
        "        if hiddens is None:\n",
        "            hiddens = [100, 100]\n",
        "        if activation == 'relu':\n",
        "            activation = nn.ReLU\n",
        "        elif activation == 'tanh':\n",
        "            activation = nn.Tanh\n",
        "        layers = [linear_init(nn.Linear(input_size, hiddens[0])), activation()]\n",
        "        for i, o in zip(hiddens[:-1], hiddens[1:]):\n",
        "            layers.append(linear_init(nn.Linear(i, o)))\n",
        "            layers.append(activation())\n",
        "        layers.append(linear_init(nn.Linear(hiddens[-1], output_size)))\n",
        "        self.mean = nn.Sequential(*layers)\n",
        "        self.sigma = nn.Parameter(torch.Tensor(output_size))\n",
        "        self.sigma.data.fill_(np.log(1))\n",
        "\n",
        "    def density(self, state):\n",
        "        state = state.to(self.device, non_blocking=True)\n",
        "        loc = self.mean(state)\n",
        "        scale = torch.exp(torch.clamp(self.sigma, min=np.log(EPSILON)))\n",
        "        return Normal(loc=loc, scale=scale)\n",
        "\n",
        "    def log_prob(self, state, action):\n",
        "        density = self.density(state)\n",
        "        return density.log_prob(action).mean(dim=1, keepdim=True)\n",
        "\n",
        "    def forward(self, state):\n",
        "        density = self.density(state)\n",
        "        action = density.sample()\n",
        "        return action"
      ],
      "metadata": {
        "id": "AlZBT7m0c4tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict(DiagNormalPolicy(2, 2).named_parameters()).keys()"
      ],
      "metadata": {
        "id": "sxMqRgK5dFLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d166d50c-001b-4c30-b6fc-4d0a22820dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['sigma', 'mean.0.weight', 'mean.0.bias', 'mean.2.weight', 'mean.2.bias', 'mean.4.weight', 'mean.4.bias'])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAML TRPO"
      ],
      "metadata": {
        "id": "4scCUfBlM4V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Trains a 2-layer MLP with MAML-TRPO.\n",
        "\n",
        "Usage:\n",
        "\n",
        "python examples/rl/maml_trpo.py\n",
        "\"\"\"\n",
        "\n",
        "import random\n",
        "from copy import deepcopy\n",
        "\n",
        "import cherry as ch\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from cherry.algorithms import a2c, trpo\n",
        "from cherry.models.robotics import LinearValue\n",
        "from torch import autograd\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
        "from tqdm import tqdm\n",
        "\n",
        "import learn2learn as l2l\n",
        "\n",
        "\n",
        "def compute_advantages(baseline, tau, gamma, rewards, dones, states, next_states):\n",
        "    # Update baseline\n",
        "    returns = ch.td.discount(gamma, rewards, dones)\n",
        "    baseline.fit(states, returns)\n",
        "    values = baseline(states)\n",
        "    next_values = baseline(next_states)\n",
        "    bootstraps = values * (1.0 - dones) + next_values * dones\n",
        "    next_value = torch.zeros(1, device=values.device)\n",
        "    return ch.pg.generalized_advantage(tau=tau,\n",
        "                                       gamma=gamma,\n",
        "                                       rewards=rewards,\n",
        "                                       dones=dones,\n",
        "                                       values=bootstraps,\n",
        "                                       next_value=next_value)\n",
        "\n",
        "\n",
        "def maml_a2c_loss(train_episodes, learner, baseline, gamma, tau):\n",
        "    # Update policy and baseline\n",
        "    states = train_episodes.state()\n",
        "    actions = train_episodes.action()\n",
        "    rewards = train_episodes.reward()\n",
        "    dones = train_episodes.done()\n",
        "    next_states = train_episodes.next_state()\n",
        "    log_probs = learner.log_prob(states, actions)\n",
        "    advantages = compute_advantages(baseline, tau, gamma, rewards,\n",
        "                                    dones, states, next_states)\n",
        "    advantages = ch.normalize(advantages).detach()\n",
        "    return a2c.policy_loss(log_probs, advantages)\n",
        "\n",
        "\n",
        "def fast_adapt_a2c(clone, train_episodes, adapt_lr, baseline, gamma, tau, first_order=False):\n",
        "    second_order = not first_order\n",
        "    loss = maml_a2c_loss(train_episodes, clone, baseline, gamma, tau)\n",
        "    gradients = autograd.grad(loss,\n",
        "                              clone.parameters(),\n",
        "                              retain_graph=second_order,\n",
        "                              create_graph=second_order)\n",
        "    return l2l.algorithms.maml.maml_update(clone, adapt_lr, gradients)\n",
        "\n",
        "\n",
        "def meta_surrogate_loss(iteration_replays, iteration_policies, policy, baseline, tau, gamma, adapt_lr):\n",
        "    mean_loss = 0.0\n",
        "    mean_kl = 0.0\n",
        "    for task_replays, old_policy in tqdm(zip(iteration_replays, iteration_policies),\n",
        "                                         total=len(iteration_replays),\n",
        "                                         desc='Surrogate Loss',\n",
        "                                         leave=False):\n",
        "        train_replays = task_replays[:-1]\n",
        "        valid_episodes = task_replays[-1]\n",
        "        new_policy = l2l.clone_module(policy)\n",
        "\n",
        "        # Fast Adapt\n",
        "        for train_episodes in train_replays:\n",
        "            new_policy = fast_adapt_a2c(new_policy, train_episodes, adapt_lr,\n",
        "                                        baseline, gamma, tau, first_order=False)\n",
        "\n",
        "        # Useful values\n",
        "        states = valid_episodes.state()\n",
        "        actions = valid_episodes.action()\n",
        "        next_states = valid_episodes.next_state()\n",
        "        rewards = valid_episodes.reward()\n",
        "        dones = valid_episodes.done()\n",
        "\n",
        "        # Compute KL\n",
        "        old_densities = old_policy.density(states)\n",
        "        new_densities = new_policy.density(states)\n",
        "        kl = kl_divergence(new_densities, old_densities).mean()\n",
        "        mean_kl += kl\n",
        "\n",
        "        # Compute Surrogate Loss\n",
        "        advantages = compute_advantages(baseline, tau, gamma, rewards, dones, states, next_states)\n",
        "        advantages = ch.normalize(advantages).detach()\n",
        "        old_log_probs = old_densities.log_prob(actions).mean(dim=1, keepdim=True).detach()\n",
        "        new_log_probs = new_densities.log_prob(actions).mean(dim=1, keepdim=True)\n",
        "        mean_loss += trpo.policy_loss(new_log_probs, old_log_probs, advantages)\n",
        "    mean_kl /= len(iteration_replays)\n",
        "    mean_loss /= len(iteration_replays)\n",
        "    return mean_loss, mean_kl\n",
        "\n",
        "\n",
        "def main(\n",
        "        env_name='HalfCheetahForwardBackward-v1',\n",
        "        adapt_lr=0.1,\n",
        "        meta_lr=1.0,\n",
        "        adapt_steps=1,\n",
        "        num_iterations=1000,\n",
        "        meta_bsz=20,\n",
        "        adapt_bsz=20,\n",
        "        tau=1.00,\n",
        "        gamma=0.95,\n",
        "        seed=42,\n",
        "        num_workers=10,\n",
        "        cuda=1,\n",
        "):\n",
        "    cuda = bool(cuda)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device_name = 'cpu'\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        device_name = 'cuda'\n",
        "    device = torch.device(device_name)\n",
        "\n",
        "    def make_env():\n",
        "        env = gym.make(env_name)\n",
        "        env = ch.envs.ActionSpaceScaler(env)\n",
        "        return env\n",
        "\n",
        "    env = l2l.gym.AsyncVectorEnv([make_env for _ in range(num_workers)])\n",
        "    env.seed(seed)\n",
        "    env.set_task(env.sample_tasks(1)[0])\n",
        "    env = ch.envs.Torch(env)\n",
        "    policy = DiagNormalPolicy(env.state_size, env.action_size, device=device)\n",
        "    if cuda:\n",
        "        policy = policy.to(device)\n",
        "    baseline = LinearValue(env.state_size, env.action_size)\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        iteration_reward = 0.0\n",
        "        iteration_replays = []\n",
        "        iteration_policies = []\n",
        "\n",
        "        for task_config in tqdm(env.sample_tasks(meta_bsz), leave=False, desc='Data'):  # Samples a new config\n",
        "            clone = deepcopy(policy)\n",
        "            env.set_task(task_config)\n",
        "            env.reset()\n",
        "            task = ch.envs.Runner(env)\n",
        "            task_replay = []\n",
        "\n",
        "            # Fast Adapt\n",
        "            for step in range(adapt_steps):\n",
        "                train_episodes = task.run(clone, episodes=adapt_bsz)\n",
        "                if cuda:\n",
        "                    train_episodes = train_episodes.to(device, non_blocking=True)\n",
        "                clone = fast_adapt_a2c(clone, train_episodes, adapt_lr,\n",
        "                                       baseline, gamma, tau, first_order=True)\n",
        "                task_replay.append(train_episodes)\n",
        "\n",
        "            # Compute Validation Loss\n",
        "            valid_episodes = task.run(clone, episodes=adapt_bsz)\n",
        "            task_replay.append(valid_episodes)\n",
        "            iteration_reward += valid_episodes.reward().sum().item() / adapt_bsz\n",
        "            iteration_replays.append(task_replay)\n",
        "            iteration_policies.append(clone)\n",
        "\n",
        "        # Print statistics\n",
        "        print('\\nIteration', iteration)\n",
        "        adaptation_reward = iteration_reward / meta_bsz\n",
        "        print('adaptation_reward', adaptation_reward)\n",
        "\n",
        "        # TRPO meta-optimization\n",
        "        backtrack_factor = 0.5\n",
        "        ls_max_steps = 15\n",
        "        max_kl = 0.01\n",
        "        if cuda:\n",
        "            policy = policy.to(device, non_blocking=True)\n",
        "            baseline = baseline.to(device, non_blocking=True)\n",
        "            iteration_replays = [[r.to(device, non_blocking=True) for r in task_replays] for task_replays in\n",
        "                                 iteration_replays]\n",
        "\n",
        "        # Compute CG step direction\n",
        "        old_loss, old_kl = meta_surrogate_loss(iteration_replays, iteration_policies, policy, baseline, tau, gamma,\n",
        "                                               adapt_lr)\n",
        "        grad = autograd.grad(old_loss,\n",
        "                             policy.parameters(),\n",
        "                             retain_graph=True)\n",
        "        grad = parameters_to_vector([g.detach() for g in grad])\n",
        "        Fvp = trpo.hessian_vector_product(old_kl, policy.parameters())\n",
        "        step = trpo.conjugate_gradient(Fvp, grad)\n",
        "        shs = 0.5 * torch.dot(step, Fvp(step))\n",
        "        lagrange_multiplier = torch.sqrt(shs / max_kl)\n",
        "        step = step / lagrange_multiplier\n",
        "        step_ = [torch.zeros_like(p.data) for p in policy.parameters()]\n",
        "        vector_to_parameters(step, step_)\n",
        "        step = step_\n",
        "        del old_kl, Fvp, grad\n",
        "        old_loss.detach_()\n",
        "\n",
        "        # Line-search\n",
        "        for ls_step in range(ls_max_steps):\n",
        "            stepsize = backtrack_factor ** ls_step * meta_lr\n",
        "            clone = deepcopy(policy)\n",
        "            for p, u in zip(clone.parameters(), step):\n",
        "                p.data.add_(-stepsize, u.data)\n",
        "            new_loss, kl = meta_surrogate_loss(iteration_replays, iteration_policies, clone, baseline, tau, gamma,\n",
        "                                               adapt_lr)\n",
        "            if new_loss < old_loss and kl < max_kl:\n",
        "                for p, u in zip(policy.parameters(), step):\n",
        "                    p.data.add_(-stepsize, u.data)\n",
        "                break"
      ],
      "metadata": {
        "id": "XXcuup_A5rdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "gdbzHe7_63lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANIL"
      ],
      "metadata": {
        "id": "iHtAReyZM8D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LAST_LAYERS = ['sigma', 'mean.4.weight', 'mean.4.bias']"
      ],
      "metadata": {
        "id": "CdWZRKVj1k7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Trains a 2-layer MLP with MAML-TRPO.\n",
        "\n",
        "Usage:\n",
        "\n",
        "python examples/rl/maml_trpo.py\n",
        "\"\"\"\n",
        "\n",
        "import random\n",
        "from copy import deepcopy\n",
        "\n",
        "import cherry as ch\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from cherry.algorithms import a2c, trpo\n",
        "from cherry.models.robotics import LinearValue\n",
        "from torch import autograd\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
        "from tqdm import tqdm\n",
        "\n",
        "import learn2learn as l2l\n",
        "\n",
        "\n",
        "def compute_advantages(baseline, tau, gamma, rewards, dones, states, next_states):\n",
        "    # Update baseline\n",
        "    returns = ch.td.discount(gamma, rewards, dones)\n",
        "    baseline.fit(states, returns)\n",
        "    values = baseline(states)\n",
        "    next_values = baseline(next_states)\n",
        "    bootstraps = values * (1.0 - dones) + next_values * dones\n",
        "    next_value = torch.zeros(1, device=values.device)\n",
        "    return ch.pg.generalized_advantage(tau=tau,\n",
        "                                       gamma=gamma,\n",
        "                                       rewards=rewards,\n",
        "                                       dones=dones,\n",
        "                                       values=bootstraps,\n",
        "                                       next_value=next_value)\n",
        "\n",
        "\n",
        "def maml_a2c_loss(train_episodes, learner, baseline, gamma, tau):\n",
        "    # Update policy and baseline\n",
        "    states = train_episodes.state()\n",
        "    actions = train_episodes.action()\n",
        "    rewards = train_episodes.reward()\n",
        "    dones = train_episodes.done()\n",
        "    next_states = train_episodes.next_state()\n",
        "    log_probs = learner.log_prob(states, actions)\n",
        "    advantages = compute_advantages(baseline, tau, gamma, rewards,\n",
        "                                    dones, states, next_states)\n",
        "    advantages = ch.normalize(advantages).detach()\n",
        "    return a2c.policy_loss(log_probs, advantages)\n",
        "\n",
        "\n",
        "def fast_adapt_a2c(clone, train_episodes, adapt_lr, baseline, gamma, tau, first_order=False):\n",
        "    second_order = not first_order\n",
        "    loss = maml_a2c_loss(train_episodes, clone, baseline, gamma, tau)\n",
        "    gradients = autograd.grad(loss,\n",
        "                              [v for n, v in clone.named_parameters() if n in LAST_LAYERS],\n",
        "                              retain_graph=second_order,\n",
        "                              create_graph=second_order)\n",
        "\n",
        "    params = [v for n, v in clone.named_parameters() if n in LAST_LAYERS]\n",
        "    for p, g in zip(params, gradients):\n",
        "            if g is not None:\n",
        "                p.update = - adapt_lr * g\n",
        "    return l2l.utils.update_module(clone)\n",
        "\n",
        "\n",
        "def meta_surrogate_loss(iteration_replays, iteration_policies, policy, baseline, tau, gamma, adapt_lr):\n",
        "    mean_loss = 0.0\n",
        "    mean_kl = 0.0\n",
        "    for task_replays, old_policy in tqdm(zip(iteration_replays, iteration_policies),\n",
        "                                         total=len(iteration_replays),\n",
        "                                         desc='Surrogate Loss',\n",
        "                                         leave=False):\n",
        "        train_replays = task_replays[:-1]\n",
        "        valid_episodes = task_replays[-1]\n",
        "        new_policy = l2l.clone_module(policy)\n",
        "\n",
        "        # Fast Adapt\n",
        "        for train_episodes in train_replays:\n",
        "            new_policy = fast_adapt_a2c(new_policy, train_episodes, adapt_lr,\n",
        "                                        baseline, gamma, tau, first_order=False)\n",
        "\n",
        "        # Useful values\n",
        "        states = valid_episodes.state()\n",
        "        actions = valid_episodes.action()\n",
        "        next_states = valid_episodes.next_state()\n",
        "        rewards = valid_episodes.reward()\n",
        "        dones = valid_episodes.done()\n",
        "\n",
        "        # Compute KL\n",
        "        old_densities = old_policy.density(states)\n",
        "        new_densities = new_policy.density(states)\n",
        "        kl = kl_divergence(new_densities, old_densities).mean()\n",
        "        mean_kl += kl\n",
        "\n",
        "        # Compute Surrogate Loss\n",
        "        advantages = compute_advantages(baseline, tau, gamma, rewards, dones, states, next_states)\n",
        "        advantages = ch.normalize(advantages).detach()\n",
        "        old_log_probs = old_densities.log_prob(actions).mean(dim=1, keepdim=True).detach()\n",
        "        new_log_probs = new_densities.log_prob(actions).mean(dim=1, keepdim=True)\n",
        "        mean_loss += trpo.policy_loss(new_log_probs, old_log_probs, advantages)\n",
        "    mean_kl /= len(iteration_replays)\n",
        "    mean_loss /= len(iteration_replays)\n",
        "    return mean_loss, mean_kl\n",
        "\n",
        "\n",
        "def main(\n",
        "        env_name='Particles2D-v1',\n",
        "        adapt_lr=0.1,\n",
        "        meta_lr=1.0,\n",
        "        adapt_steps=1,\n",
        "        num_iterations=1000,\n",
        "        meta_bsz=20,\n",
        "        adapt_bsz=20,\n",
        "        tau=1.00,\n",
        "        gamma=0.95,\n",
        "        seed=42,\n",
        "        num_workers=10,\n",
        "        cuda=0,\n",
        "):\n",
        "    cuda = bool(cuda)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    device_name = 'cpu'\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        device_name = 'cuda'\n",
        "    device = torch.device(device_name)\n",
        "\n",
        "    def make_env():\n",
        "        env = gym.make(env_name)\n",
        "        env = ch.envs.ActionSpaceScaler(env)\n",
        "        return env\n",
        "\n",
        "    env = l2l.gym.AsyncVectorEnv([make_env for _ in range(num_workers)])\n",
        "    env.seed(seed)\n",
        "    env.set_task(env.sample_tasks(1)[0])\n",
        "    env = ch.envs.Torch(env)\n",
        "    policy = DiagNormalPolicy(env.state_size, env.action_size, device=device)\n",
        "    if cuda:\n",
        "        policy = policy.to(device)\n",
        "    baseline = LinearValue(env.state_size, env.action_size)\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        iteration_reward = 0.0\n",
        "        iteration_replays = []\n",
        "        iteration_policies = []\n",
        "\n",
        "        for task_config in tqdm(env.sample_tasks(meta_bsz), leave=False, desc='Data'):  # Samples a new config\n",
        "            clone = deepcopy(policy)\n",
        "            env.set_task(task_config)\n",
        "            env.reset()\n",
        "            task = ch.envs.Runner(env)\n",
        "            task_replay = []\n",
        "\n",
        "            # Fast Adapt\n",
        "            for step in range(adapt_steps):\n",
        "                train_episodes = task.run(clone, episodes=adapt_bsz)\n",
        "                if cuda:\n",
        "                    train_episodes = train_episodes.to(device, non_blocking=True)\n",
        "                clone = fast_adapt_a2c(clone, train_episodes, adapt_lr,\n",
        "                                       baseline, gamma, tau, first_order=True)\n",
        "                task_replay.append(train_episodes)\n",
        "\n",
        "            # Compute Validation Loss\n",
        "            valid_episodes = task.run(clone, episodes=adapt_bsz)\n",
        "            task_replay.append(valid_episodes)\n",
        "            iteration_reward += valid_episodes.reward().sum().item() / adapt_bsz\n",
        "            iteration_replays.append(task_replay)\n",
        "            iteration_policies.append(clone)\n",
        "\n",
        "        # Print statistics\n",
        "        print('\\nIteration', iteration)\n",
        "        adaptation_reward = iteration_reward / meta_bsz\n",
        "        print('adaptation_reward', adaptation_reward)\n",
        "\n",
        "        # TRPO meta-optimization\n",
        "        backtrack_factor = 0.5\n",
        "        ls_max_steps = 15\n",
        "        max_kl = 0.01\n",
        "        if cuda:\n",
        "            policy = policy.to(device, non_blocking=True)\n",
        "            baseline = baseline.to(device, non_blocking=True)\n",
        "            iteration_replays = [[r.to(device, non_blocking=True) for r in task_replays] for task_replays in\n",
        "                                 iteration_replays]\n",
        "\n",
        "        # Compute CG step direction\n",
        "        old_loss, old_kl = meta_surrogate_loss(iteration_replays, iteration_policies, policy, baseline, tau, gamma,\n",
        "                                               adapt_lr)\n",
        "        grad = autograd.grad(old_loss,\n",
        "                             policy.parameters(),\n",
        "                             retain_graph=True)\n",
        "        grad = parameters_to_vector([g.detach() for g in grad])\n",
        "        Fvp = trpo.hessian_vector_product(old_kl, policy.parameters())\n",
        "        step = trpo.conjugate_gradient(Fvp, grad)\n",
        "        shs = 0.5 * torch.dot(step, Fvp(step))\n",
        "        lagrange_multiplier = torch.sqrt(shs / max_kl)\n",
        "        step = step / lagrange_multiplier\n",
        "        step_ = [torch.zeros_like(p.data) for p in policy.parameters()]\n",
        "        vector_to_parameters(step, step_)\n",
        "        step = step_\n",
        "        del old_kl, Fvp, grad\n",
        "        old_loss.detach_()\n",
        "\n",
        "        # Line-search\n",
        "        for ls_step in range(ls_max_steps):\n",
        "            stepsize = backtrack_factor ** ls_step * meta_lr\n",
        "            clone = deepcopy(policy)\n",
        "            for p, u in zip(clone.parameters(), step):\n",
        "                p.data.add_(-stepsize, u.data)\n",
        "            new_loss, kl = meta_surrogate_loss(iteration_replays, iteration_policies, clone, baseline, tau, gamma,\n",
        "                                               adapt_lr)\n",
        "            if new_loss < old_loss and kl < max_kl:\n",
        "                for p, u in zip(policy.parameters(), step):\n",
        "                    p.data.add_(-stepsize, u.data)\n",
        "                break"
      ],
      "metadata": {
        "id": "qCfEXkF0NCBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main('HalfCheetahForwardBackward-v1')"
      ],
      "metadata": {
        "id": "NerWd2cX2_rR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce104df1-e747-4421-d900-e83ccd25c86a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(\n",
            "Data:   0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/cherry/models/robotics.py:159: UserWarning: torch.lstsq is deprecated in favor of torch.linalg.lstsq and will be removed in a future PyTorch release.\n",
            "torch.linalg.lstsq has reversed arguments and does not return the QR decomposition in the returned tuple (although it returns other information about the problem).\n",
            "To get the qr decomposition consider using torch.linalg.qr.\n",
            "The returned solution in torch.lstsq stored the residuals of the solution in the last m - n columns of the returned value whenever m > n. In torch.linalg.lstsq, the residuals in the field 'residuals' of the returned named tuple.\n",
            "The unpacking of the solution, as in\n",
            "X, _ = torch.lstsq(B, A).solution[:A.size(1)]\n",
            "should be replaced with\n",
            "X = torch.linalg.lstsq(A, B).solution (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:4169.)\n",
            "  coeffs, _ = th.lstsq(b, A)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 0\n",
            "adaptation_reward -22.782715880870818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-0db55e738793>:215: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
            "  p.data.add_(-stepsize, u.data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 1\n",
            "adaptation_reward -23.009330224990844\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 2\n",
            "adaptation_reward -16.170652647018436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 3\n",
            "adaptation_reward -21.26897199630737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 4\n",
            "adaptation_reward -19.529623432159426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 5\n",
            "adaptation_reward -16.606668577194213\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 6\n",
            "adaptation_reward -19.17392887592316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 7\n",
            "adaptation_reward -16.465117950439456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 8\n",
            "adaptation_reward -14.424108867645263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 9\n",
            "adaptation_reward -14.824384279251097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 10\n",
            "adaptation_reward -13.107936692237853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 11\n",
            "adaptation_reward -21.072472431659698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 12\n",
            "adaptation_reward -13.188962420225144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 13\n",
            "adaptation_reward -14.5388640499115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 14\n",
            "adaptation_reward -19.17259264469147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 15\n",
            "adaptation_reward -14.694804224967962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 16\n",
            "adaptation_reward -14.913593361377716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 17\n",
            "adaptation_reward -16.5955118060112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 18\n",
            "adaptation_reward -8.583701906204224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 19\n",
            "adaptation_reward -27.379526901245118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 20\n",
            "adaptation_reward -12.342424597740173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 21\n",
            "adaptation_reward -16.62845694541931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 22\n",
            "adaptation_reward -10.369842104911802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 23\n",
            "adaptation_reward -13.59242642879486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 24\n",
            "adaptation_reward -14.05035891532898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 25\n",
            "adaptation_reward -12.753176555633544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 26\n",
            "adaptation_reward -9.054867296218873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 27\n",
            "adaptation_reward -12.827480416297911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 28\n",
            "adaptation_reward -10.221643092632293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 29\n",
            "adaptation_reward -11.049756345748902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 30\n",
            "adaptation_reward -10.075605893135071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 31\n",
            "adaptation_reward -8.538984255790712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 32\n",
            "adaptation_reward -10.224062418937683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 33\n",
            "adaptation_reward -9.159477802515028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 34\n",
            "adaptation_reward -5.508241901397705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 35\n",
            "adaptation_reward -8.031898117065431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 36\n",
            "adaptation_reward -5.015345349311828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 37\n",
            "adaptation_reward -4.547453441619872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Data:  30%|███       | 6/20 [00:12<00:28,  2.02s/it]"
          ]
        }
      ]
    }
  ]
}