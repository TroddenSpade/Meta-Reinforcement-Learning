{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MAML TRPO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common\n",
        "\n",
        "!apt-get install -y patchelf\n",
        "!pip install free-mujoco-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u8jyVEHU90_",
        "outputId": "6ad920d4-6675-4b6f-f6af-8036e8d88da0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libglew-dev is already the newest version (2.0.0-5).\n",
            "libgl1-mesa-dev is already the newest version (20.0.8-0ubuntu1~18.04.1).\n",
            "libgl1-mesa-glx is already the newest version (20.0.8-0ubuntu1~18.04.1).\n",
            "libosmesa6-dev is already the newest version (20.0.8-0ubuntu1~18.04.1).\n",
            "software-properties-common is already the newest version (0.96.24.32.18).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 62 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "patchelf is already the newest version (0.9-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 62 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: free-mujoco-py in /usr/local/lib/python3.7/dist-packages (2.1.6)\n",
            "Requirement already satisfied: Cython<0.30.0,>=0.29.24 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (0.29.30)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (2.19.3)\n",
            "Requirement already satisfied: glfw<2.0.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (1.21.6)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (1.15.0)\n",
            "Requirement already satisfied: fasteners==0.15 in /usr/local/lib/python3.7/dist-packages (from free-mujoco-py) (0.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fasteners==0.15->free-mujoco-py) (1.15.0)\n",
            "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.7/dist-packages (from fasteners==0.15->free-mujoco-py) (1.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.21)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.7/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (9.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cherry TRPO MAML"
      ],
      "metadata": {
        "id": "EvBkQ0-2TIGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cherry-rl learn2learn &> /dev/null"
      ],
      "metadata": {
        "id": "wZleUELMTDCf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "import cherry as ch\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from cherry.algorithms import a2c, trpo\n",
        "from cherry.models.robotics import LinearValue\n",
        "from tqdm import tqdm\n",
        "\n",
        "import learn2learn as l2l\n",
        "\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "from torch import autograd\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
        "from torch.distributions import Normal, Categorical\n"
      ],
      "metadata": {
        "id": "b1cjFZVuS3U2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EPSILON = 1e-6\n",
        "\n",
        "# def linear_init(module):\n",
        "#     if isinstance(module, nn.Linear):\n",
        "#         nn.init.xavier_uniform_(module.weight)\n",
        "#         module.bias.data.zero_()\n",
        "#     return module\n",
        "\n",
        "\n",
        "# class DiagNormalPolicy(nn.Module):\n",
        "\n",
        "#     def __init__(self, input_size, output_size, hiddens=None, activation='relu', device='cpu'):\n",
        "#         super(DiagNormalPolicy, self).__init__()\n",
        "#         self.device = device\n",
        "#         if hiddens is None:\n",
        "#             hiddens = [100, 100]\n",
        "#         if activation == 'relu':\n",
        "#             activation = nn.ReLU\n",
        "#         elif activation == 'tanh':\n",
        "#             activation = nn.Tanh\n",
        "#         layers = [linear_init(nn.Linear(input_size, hiddens[0])), activation()]\n",
        "#         for i, o in zip(hiddens[:-1], hiddens[1:]):\n",
        "#             layers.append(linear_init(nn.Linear(i, o)))\n",
        "#             layers.append(activation())\n",
        "#         layers.append(linear_init(nn.Linear(hiddens[-1], output_size)))\n",
        "#         self.mean = nn.Sequential(*layers)\n",
        "#         self.sigma = nn.Parameter(torch.Tensor(output_size))\n",
        "#         self.sigma.data.fill_(math.log(1))\n",
        "\n",
        "#     def forward(self, state):\n",
        "#         state = state.to(self.device, non_blocking=True)\n",
        "#         loc = self.mean(state)\n",
        "#         scale = torch.exp(torch.clamp(self.sigma, min=math.log(EPSILON)))\n",
        "#         return Normal(loc=loc, scale=scale)\n"
      ],
      "metadata": {
        "id": "lARajYUKTQO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer"
      ],
      "metadata": {
        "id": "9x6TJOktOIcF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, env, hidden_size=100):\n",
        "        super().__init__()\n",
        "        self.input_size = env.observation_space.shape[0]\n",
        "        self.actor_output_size = env.action_space.shape[0]\n",
        "\n",
        "        self.l1 = layer_init(nn.Linear(self.input_size, hidden_size))\n",
        "        self.l2 = layer_init(nn.Linear(hidden_size, hidden_size))\n",
        "        self.output = layer_init(nn.Linear(hidden_size, self.actor_output_size), std=0.01)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.distribution = ch.distributions.ActionDistribution(env)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.l1(x))\n",
        "        x = self.activation(self.l2(x))\n",
        "        x = self.output(x)\n",
        "        mass = self.distribution(x)\n",
        "\n",
        "        return mass"
      ],
      "metadata": {
        "id": "MDEKGt5KONHM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, env, lr, hidden_size=32):\n",
        "        super().__init__()\n",
        "        self.input_size = env.observation_space.shape[0]\n",
        "        self.critic_output_size = 1\n",
        "\n",
        "        self.l1 = layer_init(nn.Linear(self.input_size, hidden_size))\n",
        "        self.l2 = layer_init(nn.Linear(hidden_size, hidden_size))\n",
        "        self.critic_head = layer_init(nn.Linear(hidden_size, self.critic_output_size), std=1.)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr, eps=1e-5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.l1(x))\n",
        "        x = self.activation(self.l2(x))\n",
        "        value = self.critic_head(x)\n",
        "\n",
        "        return value"
      ],
      "metadata": {
        "id": "Dbv0dy_WOOf9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MAMLTRPO():\n",
        "    def __init__(self, env,\n",
        "                 actor_class=Actor, critic_class=Critic, \n",
        "                 actor_args=dict(), critic_args=dict(),\n",
        "                 adapt_lr=0.1, meta_lr=1.0, \n",
        "                 adapt_steps=1,\n",
        "                 adapt_batch_size=20, meta_batch_size=20,\n",
        "                 gamma=0.95, tau=1.0,\n",
        "                 backtrack_factor=0.5, ls_max_steps=15, max_kl=0.01,\n",
        "                 seed=42,\n",
        "                 device='cpu', name=\"MAMLTRPO\", tensorboard_log=None):\n",
        "        \n",
        "        self.device = torch.device(device)\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        if device == 'cuda':\n",
        "            torch.cuda.manual_seed(seed)\n",
        "\n",
        "        env = ch.envs.ActionSpaceScaler(env)\n",
        "        env.seed(seed)\n",
        "        env.set_task(env.sample_tasks(1)[0])\n",
        "        self.env = ch.envs.Torch(env)\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.adapt_lr = adapt_lr\n",
        "        self.meta_lr = meta_lr\n",
        "        self.adapt_steps = adapt_steps\n",
        "        self.adapt_batch_size = adapt_batch_size\n",
        "        self.meta_batch_size = meta_batch_size\n",
        "        self.backtrack_factor = backtrack_factor\n",
        "        self.ls_max_steps = ls_max_steps\n",
        "        self.max_kl = max_kl\n",
        "\n",
        "        self.policy = Actor(env, **actor_args).to(device)\n",
        "        self.baseline = Critic(env, lr=0.001, **critic_args).to(device)\n",
        "        # self.baseline = LinearValue(env.state_size, env.action_size)\n",
        "\n",
        "\n",
        "    def collect_steps(self, policy, n_episodes):\n",
        "        replay = ch.ExperienceReplay(device=self.device)\n",
        "        for i in range(n_episodes):\n",
        "            state = self.env.reset()\n",
        "\n",
        "            while True:\n",
        "                with torch.no_grad():\n",
        "                    mass = policy(state)\n",
        "                action = mass.sample()\n",
        "                log_prob = mass.log_prob(action).mean(dim=1, keepdim=True)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "                replay.append(state,\n",
        "                            action,\n",
        "                            reward,\n",
        "                            next_state,\n",
        "                            done,\n",
        "                            log_prob=log_prob)\n",
        "                \n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_state_value = self.baseline(replay[-1].next_state)\n",
        "        values = self.baseline(replay.state())\n",
        "\n",
        "        advantages = ch.generalized_advantage(self.gamma,\n",
        "                                                self.tau,\n",
        "                                                replay.reward(),\n",
        "                                                replay.done(),\n",
        "                                                values.detach(),\n",
        "                                                next_state_value)\n",
        "        returns = advantages + values.detach()\n",
        "        advantages = ch.normalize(advantages, epsilon=1e-8)\n",
        "\n",
        "        for i, sars in enumerate(replay):\n",
        "            sars.returns = returns[i]\n",
        "            sars.advantage = advantages[i]\n",
        "\n",
        "        # values_pred = self.baseline(replay.state())\n",
        "        value_loss = a2c.state_value_loss(returns, values)\n",
        "\n",
        "        self.baseline.optimizer.zero_grad()\n",
        "        value_loss.backward()\n",
        "        self.baseline.optimizer.step()\n",
        "\n",
        "        # self.baseline.fit(replay.state(), returns)\n",
        "        return replay\n",
        "\n",
        "\n",
        "    def maml_a2c_loss(self, train_episodes, learner):\n",
        "        # Update policy and baseline\n",
        "        states = train_episodes.state()\n",
        "        actions = train_episodes.action()\n",
        "        density = learner(states)\n",
        "        log_probs = density.log_prob(actions).mean(dim=1, keepdim=True)\n",
        "\n",
        "        advantages = train_episodes.advantage()\n",
        "        return a2c.policy_loss(log_probs, train_episodes.advantage())\n",
        "\n",
        "\n",
        "    def fast_adapt(self, clone, train_episodes, first_order=False):\n",
        "        second_order = not first_order\n",
        "        loss = self.maml_a2c_loss(train_episodes, clone)\n",
        "        gradients = autograd.grad(loss,\n",
        "                                clone.parameters(),\n",
        "                                retain_graph=second_order,\n",
        "                                create_graph=second_order)\n",
        "        return l2l.algorithms.maml.maml_update(clone, self.adapt_lr, gradients)\n",
        "\n",
        "\n",
        "    def meta_surrogate_loss(self, iteration_replays, iteration_policies, policy):\n",
        "        mean_loss = 0.0\n",
        "        mean_kl = 0.0\n",
        "        for task_replays, old_policy in tqdm(zip(iteration_replays, iteration_policies),\n",
        "                                            total=len(iteration_replays),\n",
        "                                            desc='Surrogate Loss',\n",
        "                                            leave=False):\n",
        "            train_replays = task_replays[:-1]\n",
        "            valid_episodes = task_replays[-1]\n",
        "            new_policy = l2l.clone_module(policy)\n",
        "\n",
        "            # Fast Adapt\n",
        "            for train_episodes in train_replays:\n",
        "                new_policy = self.fast_adapt(new_policy, train_episodes, first_order=False)\n",
        "\n",
        "            # Useful values\n",
        "            states = valid_episodes.state()\n",
        "            actions = valid_episodes.action()\n",
        "\n",
        "            # Compute KL\n",
        "            old_densities = old_policy(states)\n",
        "            new_densities = new_policy(states)\n",
        "            kl = kl_divergence(new_densities, old_densities).mean()\n",
        "            mean_kl += kl\n",
        "\n",
        "            # Compute Surrogate Loss\n",
        "            advantages = valid_episodes.advantage()\n",
        "            old_log_probs = old_densities.log_prob(actions).mean(dim=1, keepdim=True).detach()\n",
        "            new_log_probs = new_densities.log_prob(actions).mean(dim=1, keepdim=True)\n",
        "            mean_loss += trpo.policy_loss(new_log_probs, old_log_probs, advantages)\n",
        "        mean_kl /= len(iteration_replays)\n",
        "        mean_loss /= len(iteration_replays)\n",
        "        return mean_loss, mean_kl\n",
        "\n",
        "\n",
        "    def meta_optimize(self, iteration_replays, iteration_policies):\n",
        "        # Compute CG step direction\n",
        "        old_loss, old_kl = self.meta_surrogate_loss(iteration_replays, iteration_policies, self.policy)\n",
        "\n",
        "        grad = autograd.grad(old_loss,\n",
        "                                self.policy.parameters(),\n",
        "                                retain_graph=True)\n",
        "        grad = parameters_to_vector([g.detach() for g in grad])\n",
        "        Fvp = trpo.hessian_vector_product(old_kl, self.policy.parameters())\n",
        "        step = trpo.conjugate_gradient(Fvp, grad)\n",
        "        shs = 0.5 * torch.dot(step, Fvp(step))\n",
        "        lagrange_multiplier = torch.sqrt(shs / self.max_kl)\n",
        "        step = step / lagrange_multiplier\n",
        "        step_ = [torch.zeros_like(p.data) for p in self.policy.parameters()]\n",
        "        vector_to_parameters(step, step_)\n",
        "        step = step_\n",
        "        del old_kl, Fvp, grad\n",
        "        old_loss.detach_()\n",
        "\n",
        "        # Line-search\n",
        "        for ls_step in range(self.ls_max_steps):\n",
        "            stepsize = self.backtrack_factor ** ls_step * self.meta_lr\n",
        "            clone = deepcopy(self.policy)\n",
        "            for p, u in zip(clone.parameters(), step):\n",
        "                p.data.add_(-stepsize, u.data)\n",
        "            new_loss, kl = self.meta_surrogate_loss(iteration_replays, iteration_policies, clone)\n",
        "            if new_loss < old_loss and kl < self.max_kl:\n",
        "                for p, u in zip(self.policy.parameters(), step):\n",
        "                    p.data.add_(-stepsize, u.data)\n",
        "                break\n",
        "\n",
        "\n",
        "    def train(self, num_iterations=100):\n",
        "        for iteration in range(num_iterations):\n",
        "            iteration_reward = 0.0\n",
        "            iteration_replays = []\n",
        "            iteration_policies = []\n",
        "\n",
        "            for task_config in tqdm(self.env.sample_tasks(self.meta_batch_size), leave=False, desc='Data'):\n",
        "                clone = deepcopy(self.policy)\n",
        "                self.env.set_task(task_config)\n",
        "                # self.env.reset()\n",
        "                task_replay = []\n",
        "\n",
        "                # Fast Adapt\n",
        "                for step in range(self.adapt_steps):\n",
        "                    train_episodes = self.collect_steps(clone, n_episodes=self.adapt_batch_size)\n",
        "                    clone = self.fast_adapt(clone, train_episodes, first_order=True)\n",
        "                    # self.fast_adapt(clone, train_episodes, first_order=True)\n",
        "                    task_replay.append(train_episodes)\n",
        "\n",
        "                # Compute Validation Loss\n",
        "                valid_episodes = self.collect_steps(clone, n_episodes=self.adapt_batch_size)\n",
        "                task_replay.append(valid_episodes)\n",
        "                iteration_reward += valid_episodes.reward().sum().item() / self.adapt_batch_size\n",
        "                iteration_replays.append(task_replay)\n",
        "                iteration_policies.append(clone)\n",
        "\n",
        "            # Print statistics\n",
        "            print('\\nIteration', iteration)\n",
        "            adaptation_reward = iteration_reward / self.meta_batch_size\n",
        "            print('adaptation_reward', adaptation_reward)\n",
        "\n",
        "            # TRPO meta-optimization\n",
        "            # if cuda:\n",
        "            #     policy = policy.to(device, non_blocking=True)\n",
        "            #     baseline = baseline.to(device, non_blocking=True)\n",
        "            #     iteration_replays = [[r.to(device, non_blocking=True) for r in task_replays] for task_replays in\n",
        "            #                         iteration_replays]\n",
        "\n",
        "            self.meta_optimize(iteration_replays, iteration_policies)"
      ],
      "metadata": {
        "id": "YIPEWKrndmIl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Particles2D-v1')"
      ],
      "metadata": {
        "id": "Vspfc_97Pb0O"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aa = MAMLTRPO(env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhHiZBGTPkJZ",
        "outputId": "888dd69f-7386-4a49-96e8-4eda8c7852f5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aa.train(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UAU7YG-Pn34",
        "outputId": "fd8d2855-b56d-495a-ae30-d597f084f2bc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 0\n",
            "adaptation_reward -75.78358673095704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 1\n",
            "adaptation_reward -61.92253189086915\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 2\n",
            "adaptation_reward -49.71976745605469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 3\n",
            "adaptation_reward -46.61640731811524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 4\n",
            "adaptation_reward -40.19834487915039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 5\n",
            "adaptation_reward -41.34871604919432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 6\n",
            "adaptation_reward -40.66178077697754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 7\n",
            "adaptation_reward -35.9596183013916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 8\n",
            "adaptation_reward -30.60475093841552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 9\n",
            "adaptation_reward -27.88607028961182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 10\n",
            "adaptation_reward -27.51692428588867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 11\n",
            "adaptation_reward -36.65106304168701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 12\n",
            "adaptation_reward -24.712614479064946\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 13\n",
            "adaptation_reward -29.46475429534912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 14\n",
            "adaptation_reward -25.898284187316893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 15\n",
            "adaptation_reward -27.62861515045167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 16\n",
            "adaptation_reward -30.153716926574702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 17\n",
            "adaptation_reward -22.23600982666016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 18\n",
            "adaptation_reward -30.818854980468746\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 19\n",
            "adaptation_reward -31.499895591735832\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iYFpGEDjPwXy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}